\section{Revisiting the Space Classes: PSPACE vs PH}
\par Let's return to the space classes for a bit. In particular, let's consider what the complementary classes look like. In a sense, things look a lot like they do for time classes. The deterministic space classes are still obviously self dual, and for the deterministic ones, we have replaced our existential definition of acceptance with a universal one. Consider the problem $UNREACH := REACH^c$. Given a graph $G$, is it \textit{not} the case that there is a path from $1$ to $n$? That is to say, is it the case that \textit{no paths from 1 end up reaching n?} The question we want to ask next is what the space complexity of this question is. Note that if it were the case that $UNREACH \in \textbf{NL}$, then $\textbf{NL} = \textbf{coNL}$. But the implications of this would echo far and wide throughout the space hierarchy. Consider any problem in any nondeterministic space class of higher complexity than logarithmic, say \textbf{NSPACE$(f(n))$}, with $\log(n) \in O(f(n))$. That is to say, there exists a nondeterministic Turing machine which confirms membership via the existence of a path in a graph. But if $UNREACH$ were decidable in nondeterministic logarithmic space, then we could decide the complement of this problem in the same space complexity, by effectively calling the logspace algorithm $UNREACH$ on the exponentially sized graph defined by the nondeterministic Turing machine. The log and the exponential cancel out, meaning we've decided $L^c$ within the same class, and so $\textbf{NSPACE$(f(n))$} = \textbf{coNSPACE$(f(n))$}$. If $UNREACH \in \textbf{NL}$, then it would follow that \textit{all space classes are closed under complement.} This would be an incredible result if it were true, and for over 20 years, most people believed it was false, mostly by skepticism that something so sweeping could possibly be true. However, it turns out that it is indeed true:
\begin{theorem}[Immerman-Szelepcs√©nyi]
    $UNREACH \in \textbf{NL}$
\end{theorem}
\begin{proof}
    This algorithm described for this proof is in some sense very simple. However, it is very interesting in that it uses nondeterministic results \textit{conditionally} to derive further nondeterministic results. This makes the construction subtle, and at least at first glance, more than a little confusing. 
    \par The idea behind the algorithm is to first count the total number of vertices reachable by the starting vertex. This is the hard part, as we need a way to do this while being careful to only use logarithmic space. Once we have this number, call it $t$, we run through all of the vertices one at a time \textit{except for} the final vertex, and for each vertex $i$, nondeterminstically attempt to guess a path from $1$ to $i$. We then deterministically check if this path works, and if it does, we increment a counter, call it $c$, initially set to $0$. It it doesn't, we immediately halt the computation and reject. Keep in mind this is a nondeterministic machine, so there will be several computational paths in which all guesses were correct, and for these the machine will keep going. To nondeterministically accept an input, we only need a single path, so in this way we recklessly discard what isn't needed and keep going. Once we've run through each vertex, we check to see if $c = t$. If it does, then we've managed to count up to the total number of paths reachable by vertex $1$ without considering $n$ at all, and thus we can be sure that there isn't a path, and accept. Otherwise, we reject. 
    \par Now, to count to $t$. We do this via inductive counting. The induction is based on distance, so let $t_i$ denote the number of vertices reachable via a path of length at most $i$. Of course, $t_0 = 1$. Now we describe how to find $t_{i+1}$, using $t_i$. To do this, we use a 'nested for loop'. We run through all vertices $D$, and for each $D$, we define two variables $b$ and $r$, initially both $0$, and run through all vertices $E$. For each vertex $E$, we guess a path from $1$ to $E$ in at most $i$ steps, and check to see if it was valid. If the path wasn't valid, then we keep going, and if the path was valid, then we increment $r$, and then we check to see if either $D = E$ or if $(E,D)$ is an edge in the graph. If either of these conditions are met, then we set $b=1$. At the end of the inner loop, we check to see if $r < t_i$. If it isn't, then we increment $t_{i+1}$ (initially set to $0$). Else, we halt and reject. We do this for each $E$ - cycle through all of the vertices $D$, doing what we described, and either halting or moving on. Now, for a fixed $D$ since there must be a path of guesses which were all correct, we are guaranteed a nonrejecting path in which $r = t_i$. For this correct sequence of guesses, we will add $b$ to $t_{i+1}$, which is either $0$ or $1$. Through doing this, we will end up adding in all of the vertices reachable via a path of length $i$, as well as those reachable via a path of length $i+1$, thus giving us the final result we want. 
    \par This describes the algorithm. Let's confirm it works in logarithmic space. Our algorithm is split into two pieces, one for counting and one for actually confirming. Of course, the total number of vertices $t_i$ at any point is at most $n$, so can be represented via $log(n)$ bits, as can the variables $b$, $r$, $c$, and whatever counters that our Turing machine needs to execute it's for loops. Furthermore, guesses can be recorded as sequences of vertices, and this can be done in logarithmic space in the same way that we did it for $REACH$ - by recording pairs of vertices $2$ at a time, and consulting the input as we go. Thus, the final product is a logarithmic space algorithm, as desired.
\end{proof}
\begin{corollary}
    For all $f: \mathbb{N} \to \mathbb{R}$ with $\log(n) \in O(f(n))$, $\textbf{NSPACE$(f(n))$} = \textbf{coNSPACE$(f(n))$}$
\end{corollary}
\begin{corollary}
    $\textbf{NL} = \textbf{coNL}$, $\textbf{PSPACE} = \textbf{coPSPACE}$, and so forth.
\end{corollary}
The space classes then, especially those higher up then $\textbf{L}$, have shown themselves to be very sturdy, very self contained classes. $\textbf{PSPACE} = \textbf{NPSPACE} = \textbf{coNPSPACE}$ - There is just no escaping this class. Moreover, the entire polynomial hierarchy, \textit{as well as } the probabilistic and quantum polynomial time classes (to be shown later), are contained inside of \textbf{PSPACE}: Everything even remotely polynomial with respect to time is here. What we'll further see is that every structural pattern which occurs inside of $\textbf{PSPACE}$ simply repeats itself at the 'next level up', in the sense that we view $\textbf{PSPACE}$ as the 'new' \textbf{L}, and \textbf{EXP} as the 'new' \textbf{P}. Think of \textbf{PSPACE} as representing 'the final frontier' of efficiency. Any problem which can be thought of in \textit{any way} as efficient, whether it be with respect to time or space or randomness or interaction or games between two players or quantum mechanics or nondeterministic choices or complementary definitions, \textit{has to be inside of this class}. It really isn't much of a stretch to make the claim that all of complexity theory can be summed up simply as 'the study of \textbf{PSPACE}'.

Let's return to the problem REACH. It seemed like the star of the show for a while, but here we show that REACH is really a simpler version of the SAT problem in disguise. We've seen 3SAT captures all of the complexity of the SAT problem in general, and that any number greater than 3 is unnecessary. But what about numbers smaller than 3? Is there an efficient way to solve the 2SAT problem? It turns out there is.
\par The key to seeing why this is the case, is to note that for any two literals $x$ and $y$,
\[ (x \vee y) \equiv (x \Rightarrow \neg y)\]
So if we have a 2CNF expression $\phi$, then every clause can be viewed as an implication, and thus implicitly defines the edge relations of a graph, where the vertices are literals. Let's call this implicit graph $G(\phi)$. Suppose that $\phi$ is satisfiable. This property of $\phi$ should naturally translate to a property of $G(\phi)$, and indeed it does:
\begin{theorem}
    $\phi$ is satisfiable iff no literal $x$ has the property that there is a path from $x$ to $\neg x$, and also a path from $\neg x$ to $x$. 
\end{theorem}
\begin{proof}
    One might initially scratch their head and wonder why just one of these two paths isn't enough to translate to a paradoxical expression, but we can quickly alleviate this by just noting that $(x \Rightarrow \neg x) \equiv (\neg x \vee \neg x)$, i.e. to have a path like this just necessitates that the literal $x$ be false.
    \par Before we start it will be helpful to acknowledge in a vacuum that if $\phi$ is satisfiable, then all of the clauses of $\phi$ are satisfied. These clauses are of the form $(\alpha \Rightarrow \beta) \equiv (\not \alpha \vee \beta)$, and so as a property of the graph $G(\phi)$, to say that $\phi$ is satisfiable is to say that there is a truth assignment where no edge of $G$ 'goes from true to false'. For if it did, i.e. $T(\alpha)=1$ and $T(\beta)=0$, then clearly the clause would not be satisfied. Going from 'false to true' is actually fine. We just aren't allowed to ever go from 'true to false'.
    \par Suppose there exists a literal $x$ such that there is a path from $x$ to $\neg x$ and vice versa. We will show that any truth assignment $T$ will fail to satisfy $\phi$. Suppose that $T(x) = 1$. Then by definition $T(\neg x) = 0$, so there must exist an edge $(\alpha,\beta)$ along the path from $x$ to $\neg x$ such that $T(\alpha)=1$ and $T(\beta)=0$. But then we've broken our rule - our truth assignment creates an edge which goes from true to false, so it can't possible satisfy $\phi$. Alternatively if $T(x)=0$, then $T(\neg x)=1$, and an identical argument which makes use of the path from $\neg x$ to $x$ arrives at another contradiction. Thus contrapositively we have the forward direction.
    \par Conversely, suppose that there don't exist any literals with paths to and from their negation. We will describe an algorithm which constructs a satisfying truth assignment $T$. First, pick a literal $\alpha$, whose truth value has not been yet assigned, and such that there is no path from $\alpha$ to $\neg \alpha$. (WLOG this can always be done, by possibly picking $\neg \alpha$ instead.) We consider all vertices reachable by $\alpha$, and assign them to be true, including $\alpha$ itself. We also assign false to the negation of these vertices. We need to take a moment to make sure that this step is well defined: What if there is a path from $\alpha$ to both $\beta$ as well as $\neg \beta$? Suppose this is the case. Note that the graph $G(\phi)$ is symmetric in a peculiar way, via contrapositive clauses: If there is a path from $\alpha$ to $\beta$, then by following the reverse trail of contrapositives over the negations we find equivalently a path from $\neg \beta$ to $\neg \alpha$. So to have a path from $\alpha$ to $\neg \beta$ is to also have a path from $\beta$ to $\neg \alpha$. But since there is a path from $\alpha$ to $\beta$, we now also have a path from $\alpha$ to $\neg \alpha$, a contradiction by our choice of $\alpha$. Another possible way in which this step may be undefined is if we reassign a node which previously had a different assignment. What if there is a path from $\alpha$ to a vertex $\beta$ that was already assigned false in a previous step, or vice versa? If this is the case, then $\alpha$ is a predecessor to that vertex, so 'symmetrically' there would have been an assignment to the negation of alpha, by again following the reverse path of negations. So this is a delicate algorithm, highly dependent on the implicit logical structure of the graph's origins, but it is well defined nonetheless.
    \par We repeat this step until all vertices have a truth assignment. Note that it is clear by construction that we will never have an edge which goes from true to false, so this truth assignment satisfies $\phi$, completing the proof.
\end{proof}
    Thus, satisfiability of $\phi$ relies entirely on the \textit{unreachability} of $G(\phi)$. Recall that $\textbf{NL} = \textbf{coNL}$, $UNREACH := REACH^c \in \textbf{NL}$, so we can solve $2SAT$ by creating the graph $G(\phi)$, and running two subroutines of $UNREACH$. We've found a home for $2SAT$: 
\begin{corollary}
    $2SAT \in \textbf{NL}$
\end{corollary}
We can do even better though:
\begin{theorem}
    $2SAT$ is \textbf{NL}-complete.
\end{theorem}
\begin{proof}
    Note that $UNREACH$ is \textbf{NL}-complete by virtue of it being \textbf{coNL}-complete. What we effectively constructed above was a reduction from $2SAT$ to $UNREACH$, which demonstrated that it was 'easier'. What we need to do now is show that is also just as hard: We construct a reduction from $UNREACH$ to $2SAT$. We first note that the reachability problem loses no computational complexity under the assumption that the graphs being inputted are \textit{acyclic}: Any path which involves making some kind of loop works just as well without doing any looping! So just as we assume that inputs to $SAT$ are in CNF, we can assume WLOG that inputs to $REACH$ are acyclic. The construction now is the obvious one: We assign to each vertex $i$ a unique Boolean variable $x_i$, and to each edge $(i,j)$ a clause $(\neg x_i \vee x_j)$. One last thing: Where $1$ and $n$ are the start and target vertices, we assign the clauses $(x_1)$ and $(\neg x_n)$.
    \par Now we need to see that there is \textit{not} a path from $1$ to $n$ iff the expression we constructed is satisfiable. This is mostly clear from the discussion we've been having already, and from an understanting of modus ponens (hence why we added the single literal clauses). Since the graph is acyclic, we don't need to worry about about any of the problem paths from earlier: paths from $\alpha$ to $\neg \alpha$ and vice versa - since that would be a cycle. Strangely enough then, just that one little assumption is enough to ensure that without the single variable clauses, our expression would always be satisfiable.
    \par So suppose that there is a path from $1$ to $n$. Then a little thought shows that the expression we create is one which says the following:
        \begin{itemize}
            \item $x_1$
            \item $x_1 \Rightarrow x_n$
            \item $\neg x_n$
            \item Some other clauses which are guaranteed to be satisfiable by any truth assignment, independently of the other items in this list.
        \end{itemize}
    Thus it is clear that this expression is satisfiable on the sole condition that there not exist a path from $1$ to $n$, as desired.
\end{proof}
So $REACH$ was just a special case of $SAT$ in disguise. As we begin to see, nearly every decision problem is equivalent to some variant of $SAT$, whether it be a generalization (a 'hardening' of the problem) or a restriction (a 'softening').
\par Let's now turn to one of these hardenings. Does \textbf{PSPACE} have a $SAT$ of it's own? Hell yeah it does:
\begin{problem} The \textit{quantified} Boolean satisfiability problem:
    \begin{center}
        $QSAT$: Given a Boolean expression $\phi$ in CNF along with a set of Boolean variables $x_1,x_2,...,x_n$, does there exists an assignment to the variable $x_1$ such that for either assignment to the variable $x_2$, there is an assignment to the variable $x_3$,..., and so forth, up to $x_n$, such that $\phi$? Symbolically, an instance of this will look like
        \[ \exists x_1 \forall x_2 \exists x_3 ... Q x_n \phi \]
        Where $Q = \exists$ if $n$ is odd, and $\forall$ if $n$ is even.
    \end{center}
\end{problem}
Let's first see how this is indeed a further generalization from $QSAT_i$, by showing that $QSAT_i \leq QSAT$. This is by virtue of padding. Suppose we have an instance of $QSAT_i$, i.e. $\exists X_1 \forall X_2 \exists X_3... QX_i \phi$. If $X_1$ has, say, $3$ variables in it, say $X_1 = \{x_1,x_2,x_3\}$, then we can introduce $2$ new Boolean variables which won't appear in $\phi$ at all, say $y_1$ and $y_2$, and replace the single $\exists X_1$ quantification with $\exists x_1 \forall y_1 \exists x_2 \forall y_2 \exists x_3$. This will clearly turn any instance of $QSAT_i$ into an instance of $QSAT$.
\begin{theorem}
    $QSAT$ is \textbf{PSPACE}-complete.
\end{theorem}
\begin{proof}
    First we show that $QSAT \in \textbf{PSPACE}$. Consider an instance of $QSAT$ 
    \[\psi = \exists x_1 \forall x_2 \exists x_3 ... Q x_n \phi \]
    We describe a recursive algorithm $A$ which searches for a satisfying truth assignment, recycling space. We think of $A$ as a function that takes $2$ inputs, one being an instance of $QSAT$, and the other a positive integer. We will think of the initial input as $(\psi,n)$. What $A$ does first is call itself on the new input $(\psi_0,n-1)$, and await a response, where $\psi_0$ refers to $\psi$ but with one less quantifier, and the variable $x_1$ replaced by $0$ wherever it appears in $\phi$ (This isn't defined for Boolean expressions but makes sense in the context of evaluation.) If the integer in the input to $A$ is odd, then $A$ will output a 1 if either $A(\psi_0,n-1)$ or $A(\psi_1,n-1)$ outputs a 1. If the input to $A$ is even, then it outputs a $1$ only if both $A(\psi_0,n-1)$ and $A(\psi_1,n-1)$ outputs a $1$. The machine halts in acceptance iff $A(\psi,n)$ outputs a $1$. The base case is reached when $n=0$, i.e. we've fixed a truth assignment for each quantifier. At this point, we resort to a polynomial space algorithm for $SAT$, searching for a truth assignment to the rest of the variables in $\phi$ which, along with the fixed ones corresponding to the quantifiers, satisfied $\phi$. If $\phi$ is satisfiable, then $A(\psi,0) = 1$, else it's $0$. As long as we continually reuse the space used by our $SAT$ algorithm, we only need to keep track of at most linear number of constant values, depending on our recursion depth, so this algorithm clearly works in polynomial space.
    \par To show that all \textbf{PSPACE} problems reduce to $QSAT$, let $L$ be decidable by a Turing machine $M$ which uses polynomial space for a computation on $x$. Let $|x|=n$, and consider the configuration graph $G_x$. If the machine runs in space $n^k$, then we can assume that the configurations of our graph can be encoded using a polynomial number of bits, say $n^k$.
    \par Let $P(a,b,i)$ be a predicate which is true iff there exists a path in $G_x$ from configuration $a$ to configuration $b$ of length $2^i$. We will define for predicates of this form Boolean expressions $\psi_i$ with free variables in the disjoint sets $A = \{a_1,a_2,...,a_{n^k}\}$, and $B = \{b_1,b_2,...,b_{n^k}\}$. Note this isn't all of the free variables of $\psi_i$, just a special set of them. Consider fixing truth assignments to $A$ and $B$. We will construct $\psi_i$ in such a way that $\psi_i$ is satisfiable iff the configurations $a$ encoded in binary by the truth assignment to $A$ has a path to the configuration $b$ encoded in binary by the truth assignment to $B$, of length less than or equal to $2^i$. Then we can note that if $I$ is the binary encoding of the initial configuration and $F$ is the binary encoding of the final configuration (assuming WLOG that it is unique), then 
    \[ x \in L \iff \psi_{n^k}(I,F) \]
    We build up the $\psi_i$'s inductively. For $i=0$, $\psi_0(A,B)$ is the statement that either $a_i = b_i$ for all $i=1,...,n^k$, or configuration $B$ is yielded by $A$ in one step. First, then, we need a Boolean expression which is true precisely when $a_i = b_i$ for all $i$. For each $i$, $a_i=b_i \equiv (a_i \wedge b_i) \vee (\neg a_i \wedge \neg b_i)$. Thus the assertion that configurations $a$ and $b$ are the same is represented by
    \[ \bigwedge_{i=1}^{n^k}a_i = b_i \equiv \bigwedge_{i=1}^{n^k}(a_i \wedge b_i) \vee (\neg a_i \wedge \neg b_i) \]
    Now we need a Boolean expression for $a$ yielding $b$ in one step. To do this we default to our earlier proof of the Cook-Levin theorem. There we showed essentially that a single bit of the configuration can be seen as a very large Boolean expression involving a subset of the $a_i$ \textit{constant with respect to $n$}. That is to say, no matter what the length of the input $x$ was, this Boolean expression, though long and unwieldy, stays the same length. If we let $m_i$ denote the $i^{th}$ bit of the configuration yielded by $a$, then we can see that the assertion that $a$ yields $b$ in one step is equivalent to the statement that $m_i = b_i$ for each $i$, and this can be written the same way that we wrote $a_i=b_i$. The final statement $\psi_0(A,B)$ is then the OR of the two expressions we just described, and can be seen to be length $O(n^k)$. Since we are going for a reduction, it is worth noting here that the map taking configurations $A$ and $B$ to $\psi_0(A,B)$ can be computed in logarithmic space, by again defaulting to our proof of Cook-Levin.
    \par Now for the inductive step, we attempt to break apart $\psi_i+1$ as the AND of two $\psi_i$ expressions, just as we broke apart the $PATH$ predicate in the proof of Savitch's Theorem. It is clearly true that
    \[ \psi_{i+1}(A,B) \iff \exists Z [\psi_i(A,Z) \wedge \psi_i(Z,B)] \]
    This is essentially the idea but in it's current implementation, doesn't work. If we wrote $\psi_i$ this way then the expressions would quickly become too long - growing exponentially by doubling with each successive $i$, and thus surely leaving our final expression $\psi_{n^k}(I,F)$ exponential in $|x|$. We need to get by using only a single $\psi_i$, and we can accomplish this by using additional quantifiers:
    \[  \psi_{i+1}(A,B) \iff \exists Z \forall X \forall Y [((X=A \wedge Y=Z) \vee (X = Z \wedge Y=B) \Rightarrow \psi_i(X,Y)] \]
    Note that through the introduction of $X,Y,$ and $Z$ we have implicitly defined $3n^k$ new literals which will be free variables in the expression $\psi_i$. This step is crucial in the exact same way that reusing space by erasing our previous triples was crucial in the proof of Savitch's Theorem. Note now that given two configurations $A$ and $B$, $\psi_i(A,B)$ can be computed in logarithmic space, by virtue of $\psi_0$ being computable in logarithmic space. 
    \par This completes our definition of $\psi_{i+1}$, and by induction completes the definition of $\psi_i$ for all $i$. By the fact that all quantified expressions can be written in prenex normal form, and this simplification can easily be done in polynomial time. We technically still need to make sure that $\psi_i$ is in CNF, which is a nontrivial consideration, but if I have any self control at all then I will be dead before I bother to fill that in and work through those details.
    \par This is then a reduction from an input $x$ to a quantified boolean expression $\phi = \psi_{|x|^k}(I_x,F)$, which involves a polynomial number of quantifiers (recall the $X,Y,Z$ configurations - these correspond to $3n^k$ free variables each to be quantified over, and there are $log(n^k)=klog(n)$ many of them). Then clearly $x \in L$ iff $\phi$ is satisfiable in the manner described by the problem $QSAT$.  
\end{proof}
What is the difference between $QSAT$ and $QSAT_i$? The difference is the number of turns. With $QSAT$, the number of alternations of quantifiers is allowed to vary from input to input. To define a reduction to $QSAT_i$ is to associate with every input a game for which the number of turns is a fixed, bounded number. $|x|$ could be an enormous length, hundreds and hundreds of characters long, but the number of turns of the game that $x$ is mapped to still must remain less than or equal to $i$. To define a reduction to $QSAT$ is to associate with every input a game whose length is allowed to vary. The only restriction is that the number of turns needs to be reasonable - polynomial in $|x|$. How much could this actually matter? Apparently, it matters a lot. Note the following fact about \textbf{PH}.
\begin{theorem}
    If \textbf{PH} has a complete problem, then it collapses to a finite level - i.e. there exists an $i$ with $\textbf{PH} = \Sigma_i^{\textbf{P}}$. 
\end{theorem}
\begin{proof}
    Let $L$ be \textbf{PH}-complete. Then $L \in \textbf{PH}$, obviously. So $L \in \Sigma_i^{\textbf{P}}$ for some $i \geq 1$. But then $QSAT_i$ is complete in $L \in \Sigma_i^{\textbf{P}}$, so for any language $L' \in \textbf{PH}$, $L' \leq L \leq QSAT_i \in \Sigma_i^{\textbf{P}}$. Thus, $\textbf{PH} \subseteq \Sigma_i^{\textbf{P}}$.
\end{proof}
But we now know that \textbf{PSPACE} does have a complete problem! This leaves us with a very interesting result:
\begin{fact}
    If \textbf{PH} = \textbf{PSPACE}, then \textbf{PH} collapses.
\end{fact}
Are there any known 'layers' between \textbf{PH} and \textbf{PSPACE}? Yes. Plenty, in fact, though mostly related to each other by a specific theme. We'll get to those later. More interestingly, the \textit{quantum} version of \textbf{P}, which we'll get to soon, not only lives inside of \textbf{PSPACE}, but has a lot of strong evidence to indicate that it doesn't even contain \textit{any} level of \textbf{PH} in it's entirety, aside from \textbf{P}, meaning that it intersects all of them, but appears comparable to none of them. \textbf{BQP} is truly fascinating in this regard.

