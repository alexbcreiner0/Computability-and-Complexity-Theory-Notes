\section{The Structure of Complexity Classes}
Here we take a step back, attempting to define complexity classes more rigorously and taking a look at the basic facts which are true of all of them. Since we are about to attempt to be more rigorous, some additional conventions are in order. We assume in this section that all Turing machines are over the fixed minimal alphabet $\{0,1\}$. We also fix once and for all an enumeration of the finite strings in this alphabet, so that we may refer to inputs to Turing machines as numbers, or talk of 'the $k^{th}$ string'. Note that with this fixed enumeration, languages may be thought of interchangeably as sets of strings or sets of natural numbers. In fact, if we want to think about languages as singular objects instead of as sets of objects, we may choose to view languages as infinite binary strings, with the interpretation that if the $k^{th}$ character is a $1$, then the $k^{th}$ string is in the language.
\par \textbf{Cantor space} is the natural topology to impose on infinite binary strings. It is homeomorphic to the Cantor middle thirds set as a subspace of the real number line with the standard metric topology, and is also homeomorphic to the product space $2^{\omega}$. It is a perfect Polish space (that is, separable and completele metrizable with no isolated points) which is also compact and totally disconnected, and in fact any topological space with these properties is homeomorphic to it. The standard metric is for Cantor space is noteworthy here - it is the metric in which closeness of sequences is determined by how many initial digits the sequences have in common. Languages are naturally sequences, and Cantor space is the natural space that these objects live in. Finite binary strings are the inputs to machines, where subsets of these are naturally infinite binary strings - decision problems, and subsets of decision problems - complexity classes, are subsets of Cantor space. 
\par Not all sets of languages should be complexity classes though. In descriptive set theory, it is typical to restrict the sets of interest to be those which are closed under continuous pre-images. We will attempt to do something similar for complexity classes.
 \par We define a \textbf{complexity class} to be a nonempty subset $\textbf{C} \subseteq \textbf{R}$ of decision problems which is closed under log-space reductions. That is, if $f:\{0,1\}^* \to \{0,1\}^*$ is log space computable, and $L_1,L_2$ are language such that $L_2 \in \textbf{C}$ and $n \in L_1 \iff f(n) \in L_2$, then $L_1 \subseteq \textbf{C}$. Since not every computable function is log space computable, complexity classes are not necessarily pointclasses in the lightface sense. (I would like to show this and maybe explore it a little.) \textbf{P}, \textbf{NP}. \textbf{BPP}, \textbf{PSPACE}, \textbf{NL}, and so forth are all complexity classes via this definition.
\par I've never seen a textbook formally define a complexity class this way, and the reason is clear. I chose log-space reductions in the definition because it would serve well for a larger set of classes than choosing poly-time reductions. For instance, soon I'll show that no nontrivial proper subset of \textbf{P} can possibly be a complexity class under a poly-time definition. Furthermore, sticking exclusively with log-space reductions isn't perfect either. If a set is closed under polynomial time reductions, then it is closed under log-space reductions, but the converse is false unless \textbf{L} = \textbf{P}, and so there are likely complexity classes under the log space definition which would fail to be remain classes under the poly-time definition. 
\par There is a special very small class, \textbf{FO}, the class of boolean first order queries, which is known to be properly contained in \textbf{L}. This class is also known to be equal to $\textbf{AC}^0$, the class of problems which are decidable by polynomial sized families of boolean circuits (one for each input length) of constant depth, but unbounded $FAN-IN$, and is pretty much the smallest class which anyone cares about. To define complexity classes to be closed under $\textbf{AC}^0$ reductions would be an even finer definition, and the sensibly 'smallest' nontrivial complexity class would be a complexity class under this definition. However, since $\textbf{AC}^0$ is proven to be proper in \textbf{L}, most if not all classes containing \textbf{L} would fail to be complexity classes.
\par The best course of action would probably to define complexity classes conditionally, in terms of their size. For sets containing \textbf{P}, define complexity classes to be sets closed under poly-time reductions. For sets containing \textbf{L} but not contained in \textbf{P}, define complexity classes to be sets closed under log-space reductions, and for sets containing $\textbf{AC}^0$ but not contained in \textbf{L}, define complexity classes to be sets which are closed under $\textbf{AC}^0$ reductions. Here I take the log-space reductions definition as the exclusive one, since that's all we need, and point out what would change under the poly-time definition. I'd be curious what sticks in the $\textbf{AC}^0$ definition.
\begin{lemma}
	Every complexity class except for $\{\varnothing\}$ contains an infinite set, namely $\mathbb{N}$ (or alternatively $\{0,1\}^*$ or even more alternatively the sequence $111\ldots$). Furthermore, every complexity class except for the one above as well as $\{\varnothing,\mathbb{N}\}$ and $\{\mathbb{N}\}$ contains every finite set, and is thus countably infinite. We will refer to any complexity class which isn't one of these three as \textbf{nontrivial}. We will also refer to sets which aren't $\varnothing$ or $\mathbb{N}$ as nontrivial.
\end{lemma} 
\begin{proof}
	Suppose $\textbf{C} \neq \{\varnothing\}$ a complexity class. Let $L \in \textbf{C}$, and $x \in L$. Consider the 'always yes' relation $\mathbb{N}$, and define the function $f$ to simply be the constant function $f(n) = x$. This is clearly computable in log-space, and clearly $n \in \mathbb{N} \iff f(n) \in L$. Thus by closure under reductions it must be that $\mathbb{N} \in \textbf{C}$. 
	\par Now let $L$ be a nontrivial complexity class, let $L \in \textbf{C}$ be a nontrivial language with $y_1 \in L, y_0 \notin L$, and let $F = \{x_1,x_2,...,x_n\}$ be an arbitrary finite language. Define the function $f$ by $f(x_i) = y_1$ for $i=1,...,n$, and $f(x) = y_0$ otherwise. This function is computable in log-space, because the finite nature of the mapping means all of the 'computation' of the function can be hardcoded into a Turing machine via a finite set of states - we only need space to write the final answer, which doesn't count as space use, and time to read the input. Furthermore it is clear that $x \in F \iff f(x) \in L$, so by closure under reductions it follows that $F \in \textbf{C}$. Thus $\textbf{C}$ is infinite, and that it is countable is inherited from $\textbf{R}$ being countable.
\end{proof}
\begin{lemma}
	If $\textbf{C}$ is a nontrivial complexity class with a complete set, then that complete set cannot be trivial. Furthermore, there exists a complexity class with no complete sets.
\end{lemma}
\begin{proof}
	Intuitively, this is because $\varnothing$ and $\mathbb{N}$ represent the 'always no' and 'always yes' relations, respectively. To have a reduction from a language $L$ to say, $\mathbb{N}$ would be to say that $L$ itself is $\mathbb{N}$, since $f(x) \in \mathbb{N} \Rightarrow x \in L$, but $f(x) \in \mathbb{N}$ is true all of the time. So the only language which can reduce to $\mathbb{N}$ is $\mathbb{N}$ itself, and the same is true of $\varnothing$ by an identical argument. It follows that the only complexity class for which $\mathbb{N}$ is complete is $\{\mathbb{N}\}$, and the only complexity class for which $\varnothing$ is complete is $\{\varnothing\}$.
	\par The easy example of a complexity class with no complete set is the third trivial class, $\{\varnothing,\mathbb{N}\}$. This follows from the previous paragraph - there is no way to reduce $\varnothing$ to $\mathbb{N}$, and vice versa, so neither can be complete. 
\end{proof}
\begin{lemma}
	All nontrivial sets in \textbf{L} are \textbf{L}-complete. (If complexity classes were defined in terms of polynomial time reductions, then \textbf{L} would be replaced with \textbf{P}.)
\end{lemma}
\begin{proof}
	Let $E$ be a nontrivial set in \textbf{L}, and let $L \in \textbf{L}$ be arbitrary. Let $M_L$ be the log-space Turing machines deciding $L$ and $E$. Let $x_1 \in E$ and $x_0 \notin E$. Define the mapping $f$ by
	\[ f(x) = \begin{cases}
			  	x_1 & \textrm{if $M_L(x)$ halts in acceptance} \\
			  	x_0 & \textrm{otherwise}
			  \end{cases} \] 
Clearly $x \in L \iff f(x) \in E$, and clearly $f$ is log-space computable by virtue of $L$ iteself being log-space computable. Thus this is a very stupid reduction from $L$ to $E$. 
\end{proof}
\begin{lemma}
	\textbf{L} is the smallest nontrivial complexity class under our definition. (Again, under a polynomial time reduction definition of complexity class, \textbf{P} would replace \textbf{L}.)
\end{lemma}
\begin{proof}
	Suppose $\textbf{C} \subseteq \textbf{L}$ be nontrivial. Let $L \in \textbf{C}$ be nontrivial. Then $L \in \textbf{L}$, so $L$ is \textbf{L} complete. Thus any language $L' \in \textbf{L}$ must reduce to $L$, and so by closure under reductions it must follow that $L' \in \textbf{C}$. Thus $\textbf{L} \subseteq \textbf{C}$, i.e. $\textbf{L} = \textbf{C}$.
\end{proof}
\begin{lemma}
 If $\textbf{C}$ is a nontrivial complexity class with a finite or a cofinite complete problem, then $\textbf{C} = \textbf{L}$. 
\end{lemma}
\begin{proof}
	Let $M$ be a finite complete problem for a nontrivial complexity class \textbf{C}. Note that any finite problem can be solved in log-space (and linear time), via just 'hardcoding' computations into the states of a Turing machine. The only computational resource used is the time required to scan the input. Thus, any problem in the class \textbf{C} can be simply reduced to $M$ in log-space, and then solved in log-space. Thus, any problem in $\textbf{C}$ can be solved via two sequential log-space reductions, and this can be remade into one overall log-space computation by our results, and so $\textbf{C} \subseteq \textbf{L}$, but since \textbf{L} is the smallest nontrivial complexity class, we must have that $\textbf{C} = \textbf{L}$.
	\par Next suppose that \textbf{C} is a nontrivial complexity class with a cofinite complete problem $E$. Then $E^c$ would be finite and complete for \textbf{coC}, meaning that $\textbf{coC} = \textbf{L}$, and since $\textbf{L}$ is self dual, this would imply that $\textbf{C} = \textbf{L}$.
\end{proof}
The reason for all this was to derive the minimal requirements for a class which are needed to assume that complete problems must be infinite and coinfinite, and we now have that:
\begin{corollary}
	Let $\textbf{C}$ be a complexity class properly containing \textbf{L}. Then any complete problem for \textbf{C} must be both infinite and coinfinite. Furthermore, every nontrivial complexity class has an infinite complete problem which isn't $\mathbb{N}$.
\end{corollary}
\begin{proof}
	If $E$ were a complete problem for such a class, then assuming it were finite would require that it be equal to \textbf{L} by the above results, a contradiction. Furthermore, it goes without saying that \textbf{L} has nontrivial infinite problems.
\end{proof}
It is sort of interesting to know that \textbf{L} is the only complexity class which can have finite complete problems. To prove $\textbf{P} \neq \textbf{L}$, it suffices to show that no finite problem in \textbf{P} can be \textbf{P} complete.
\begin{lemma}
	Let \textbf{C} be a nontrivial complexity class, and let $E$ be an infinite \textbf{C} complete problem. Then $E-F$ is still complete for any finite $F \subseteq E$, and with the additional assumption that $E$ is not cofinite, we also have that $E \cup G$ is still complete for any finite set $G$
\end{lemma}
\begin{proof}
	Note that in the statement I get to assume that my arbitrary class has an infinite complete problem, but I can't hold $E$ completely arbitrary because in the case that $\textbf{C} = \textbf{L}$ it may be the case that $E$ is finite. So I got some but not all of what I wanted.
	\par Let $L$ be an arbitrary language in \textbf{C}, and $f$ be the reduction from $L$ to $E$. For the case of $E-F$, since $E$ is infinite $E-F$ is still itself infinite. Fix $x_1 \in E-F$. Note that since $F$ is finite, we can compute easily in log space whether or not $f(x) \in F$. Define
	\[ m(x) = \begin{cases}
				x_1 & \textrm{if $f(x) \in F$} \\
				f(x) & \textrm{otherwise}
				\end{cases} \]
Since $f$ is log space computable, we can compute $m$ by computing $f$, and then compute $m$ conditionally on the subroutine deciding if $m \in F$. (This requires that lemma about composition of log space functions, but it works.) So $m$ is log space computable. If $x \in L$ and $f(x) \in F$, then $m(x) = x_1 \in E-F$, so $m(x) \in E-F$. If $x \in L$ and $f(x) \notin F$, then $x \in L \implies m(x) = f(x) \in E-F$, so either way we have $x \in L \implies m(x) \in E-F$. Conversely, if $x \notin L$, then $f(x) \notin E$, and so $f(x) \notin F$, i.e. $m(x) = f(x) \notin E$. So $m$ is a log space reduction from $L$ to $E-F$, and thus $E-F$ is complete.
\par Next, let $G$ be finite, and without loss of generality, assume it is disjoint from $E$. (If it isn't, then we just replace $G$ with $G-E$ and repeat the following argument.) Consider $E \cup G$. Let $L$ and $f$ be as they were before. Also similar to before, we note that in log space it can easily be computed if a string belongs to $G$. Since $E$ is not cofinite, we can choose an $x_0 \notin E \cup G$. Define 
\[ p(x) = \begin{cases}
			x_0 & \textrm{if $f(x) \in G$} \\
			f(x) & \textrm{otherwise}
\end{cases} \]   
By the same argument as above, $p$ is easily seen to be log-space computable. If $x \in L$ then $f(x) \in E$, so it cannot be the case that $f(x) \in G$ since $E \cap G = \varnothing$. Thus $m(x) = f(x) \in E \cup G$. If $x \notin L$, then we have two cases. If $f(x) \in G$, then $m(x) = x_0 \notin E \cup G$, and if $f(x) \notin G$, then since $f(x) \notin E$, we have that $m(x) = f(x) \notin E \cup G$. The proof is complete.
\end{proof}
\par The above results give us some comfort in knowing basic things that can be thought about complete problems for complexity classes. They are, in a sense, big and round. This gives some intuition into arguments like the following, which shows that if $\textbf{P} \neq \textbf{NP}$, then there must exist \textbf{NP}-intermediate problems - problems which are neither \textbf{P} nor \textbf{NP}-complete.
\begin{theorem}[Ladner's Theorem]
	If $\textbf{P} \neq \textbf{NP}$, then there exists a problem $A \in \textbf{NP}$ which is not in \textbf{P} yet not \textbf{NP}-complete.
\end{theorem}
\begin{proof}
	We begin with $SAT$. What we will do is rather interesting. We will remove points from $SAT$ strategically. By the end, it will be tempting to say that we removed 'enough' strings from $SAT$ to no longer be $\textbf{NP}$-complete, but 'not enough' to drop down to \textbf{P}.
	\par In particular, we will define a function $f:\mathbb{N} \to \mathbb{N}$, and then use this to define 
	\[ A = \{x: x \in SAT \wedge f(|x|) \textrm{ is even}\}  \] 
	Note that for this problem to be in \textbf{NP}, it suffices to make sure that $f$ is poly-time computable. As long as this is true, then we can decide $A$ using the typical guessing machine for $SAT$, and then before normally halting and accepting, carrying out the deterministic computation to see if $f(|x|)$ is even.
	\par Our construction of $f$ employs a diagonal argument. Let $\{M_i\}_{n \in \omega}$ be an enumeration of Turing machines which are 'clocked' such that for any $i$, $M_i$ operates in time $|x|^i$, and such that $\{L(M_i)\}_{n \in \omega} = \textbf{P}$. It might not seem it at first, but this set is recursively enumerable. We get away with this because we are arbitrarily halting the computation (say, in rejection) of the machine after $|x|^i$ many steps. As long as every machine occurs infinitely often in the list, then we will eventually reach the bound of any polynomially bounded machine, and this gives us all of \textbf{P}. We also define a similar enumeration $\{f_i\}_{i \in \omega}$ of all polynomial time computable functions. What we need diagonalize out of both of these sets - we need to make sure that none of the Turing machines in our enumeration can decide $A$, and we also need to make sure that none of the functions $f_i$ will work as reductions from $SAT$ to $A$.
	\par We define $f$ inductively. First, $f(0) = f(1) = 2$. For $n \geq 1$ we define $f(n+1)$ as follows. For reasons which will become apparent, we want the function $f$ to grow \textit{hella slow}. It will go to infinity, but will barely \textit{ever} increase. We do this by defining $f(n+1) = f(n)$ as long as $(\log(n))^{f(n)} \geq n$. What this means is that $f(n)$ stays the same until it's been asleep for so long that linear growth has managed to outpace $(\log(n))^{f(n)}$, before waking up and doing something, \textit{maybe}. Once it wakes up, we have two cases:
	\par It's been even. That is, $f(n) = 2i$, then first check to see if there is an input $x$ with $|x| \leq \log(n)$ such that either $M_i(x)$ accepts and $x \notin A$ (i.e. $f(|x|)$ is odd or $f(x) \notin SAT$) or $M_i(x)$ rejects and $x \in A$ (i.e. $f(|x|)$ is even and $f(x) \in SAT$). If it is, then we let $f(n+1) = f(n)+1$, and otherwise we let $f(n+1) = f(n)$. This way, we are diagonalizing out of $A$ on the even cases.
	\par Next, suppose we are at an odd case, that is $f(n) = 2i+1$. Here we diagonalize out of the set of poly time functions $f_i$. We want to make sure that none of these defines a reduction from $SAT$. To this end, we check to see if there exists an $x$ of length $|x| \leq \log(n)$ such that either $x \in SAT$ and $f_i(x) \notin A$ (i.e. either $f(|f_i(x)|)$ is odd or $f_i(x) \notin SAT$) or $x \notin SAT$ and $f_i(x) \in A$ (i.e. $f(|f_i(x)|)$ is even and $f_i(x) \in SAT$). If such an $x$ exists, we define $f(n+1) = f(n)+1$, and otherwise we let $f(n+1) = f(n)$. In this way, we are simultaneously diagonalizing out of the polynomial time Turing machines, as well as the polynomial time reductions, so that no polynomial time Turing machine can decide $A$, and no polynomial time reduction can reduce $SAT$ to $A$. 
	\par It remains to show that $A \in \textbf{NP}$, which recall we said would be the case if $f$ was polynomial time computable. This was the reason we had $f$ grow extremely slow. We describe a polynomial tine Turing machine which computes $f$. Fix $S$ to be a deterministic Turing machine which decides $SAT$, by just running through all possible truth assignments. Obviously this runs in time $O(2^n)$. Suppose we wish to compute $f(n+1)$. First, the machine computes $(\log(n))^{f(n)}$. Assuming inductively that $f(n)=m$ can be computed in poly-time, it is fixed, and $(\log(n))^{m} \leq n^m$ is a slow growing function which can presumably be computed quickly (I should show that $n \mapsto \log(n)$ is polynomial time computable.) This is all that needs to be done unless the calculated number is smaller than $n$. 
	\par If the calculated number is smaller than $n$ and $f(n)$ is even (this can of course be done quickly by simply checking the last digit of $n$), then we need to do a lot. First we recover $i$ by dividing by $2$, which can easily be done in linear time by the classic long division algorithm we do on paper. After we have $i$, we need to work through all inputs $x$ of length $|x| \leq \log(n)$, of which there are $n$, and for each of these inputs we need to simulate the machine $M_i(x)$. This can be done in time 
	\[ |x|^i \leq \log(n)^i \leq \log(n)^{f(n)} < n \]
Thus, linear time! Similarly, we can use our machine $S$ to decide membership in sat, since $|x| \leq \log(n)$ means $S$ runs in time $O(n)$. So in the case of $f(n)$ even, we can still perform the computation of $f(n)$ in polynomial time. The case in which $f(n)$ is odd is an identical argument. Thus, $f$ is computable in polynomial time.
	\par To confirm the construction works as desired, suppose that $A$ is in \textbf{P}. Then there would never be an $i$ such that $M_i$ disagrees with $A$ for some $i$, and so the function $f$ would end up being constant $f(n) = 2$. But then by definition of $A$, we would have $A = SAT$, and so $SAT \in P$, contradicting the assumption that $\textbf{P} \neq \textbf{NP}$. Suppose next that $A \notin \textbf{P}$, but $SAT$ has a polynomial time reduction to $A$. Then by the same argument, $f$ is eventually just constant $f(n)=3$. This forces the set $A$ to be finite, so $A \in \textbf{P}$, another contradiction. Thus, $A$ is \textbf{NP}-intermediate.
\end{proof} 
$A$ is a very contrived example of an \textbf{NP}-intermediate problem. Are there any more natural candidates? There are in fact two, all candidates for very different reasons. Both reasons require theory these notes haven't gotten to yet, but we'll state them here anyway.
\begin{itemize}
	\item The $FACTORING$ problem. If it were \textbf{NP}-complete, then it would be the case that $\textbf{NP} \subseteq \textbf{BQP}$. This is unlikely to be true, as there is a known quantum algorithm called Grover Search which can increase the speed of \textbf{NP}-complete problems, but the improvement is only quadratic, and moreover this algorithm has been proven \textit{optimal} in the case of problems for which guessing and checking really is the only option. Thus \textbf{NP}-complete problems which would take an exponential number of steps to decide deterministically would still be exponential time for a quantum computer. There is a wealth of evidence aside from this to suggest that \textbf{BQP} is incomparable to every class in the polynomial hierarchy, and to have the factoring problem be \textbf{NP} complete would contradict all of it.
	\item The $GRAPH-ISOMORPHISM$ problem. (Given two graphs, are they isomorphic?) If this were \textbf{NP}-complete, then it's complementary problem, $GRAPH-NONISOMORPHISM$, would be \textbf{coNP} complete. However, this complementary problem is a very natural example of a problem which is solvable efficiently by an Arthur-Merlin protocol, meaning $GRAPH-NONISOMORPHISM \in \textbf{AM}$. Thus it would follow that $\textbf{coNP} \subseteq \textbf{AM}$, as we will show later, this implies a collapse of the polynomial hierarchy at the second level. The assumption that \textbf{PH} does not collapse is the natural extension of the conjecture that $\textbf{P} \neq \textbf{NP}$, so most would conjecture that this is unlikely.
\end{itemize}

